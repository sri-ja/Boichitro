{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fumxauPmuoe6",
        "epcvGEbsusJQ",
        "-czshxXPu0Mh",
        "HjaKwjVCu9Xn",
        "mH8RLDDWvKYd",
        "1o_yaZUdxids",
        "i1kOfUgwxl9w",
        "--99GYzVx0eG",
        "OVJBXFi2x44b",
        "6g_emoxeym5N",
        "7hi10n3jyv4j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "MoOlk7sUEzRD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTN7DZyZDTSR"
      },
      "outputs": [],
      "source": [
        "!pip3 install beautifulsoup4\n",
        "!pip3 install requests\n",
        "!pip3 install pandas\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "!pip3 install inltk\n",
        "!pip3 install bnlp_toolkit\n",
        "!pip3 install wordcloud\n",
        "!pip3 install git+https://github.com/banglakit/lemmatizer.git#egg=banglakit-lemmatizer\n",
        "!pip3 install emoji \n",
        "\n",
        "\n",
        "!git clone https://github.com/Foysal87/bn_nlp\n",
        "!git clone https://github.com/banglakit/lemmatizer\n",
        "# need to include the model for pos tagging manually because github keeps throwing an error whenever i try to do so, the model can be found here - https://github.com/sagorbrur/bnlp/blob/master/model/bn_pos.pkl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from pprint import pprint\n",
        "import bn_nlp\n",
        "import emoji\n",
        "import regex\n",
        "import re"
      ],
      "metadata": {
        "id": "N6MqGz63EqBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inltk.inltk import setup\n",
        "setup('bn')"
      ],
      "metadata": {
        "id": "S20eK6aVEuYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the file to be analysed"
      ],
      "metadata": {
        "id": "u0TIQFcTWCL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reqfile = '/content/filename.txt' # change file name according to whatever you are trying to study and analyse properly"
      ],
      "metadata": {
        "id": "W6LLsGikWNIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting the number of non-dictionary words, which here are emojis\n"
      ],
      "metadata": {
        "id": "QqsMKHjME9aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)"
      ],
      "metadata": {
        "id": "fPv_rkzVTJ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(reqfile, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "emojis = 0\n",
        "for line in lines:\n",
        "  emojis = emojis + len(re.findall(emoj, line))\n",
        "\n",
        "print(emojis)"
      ],
      "metadata": {
        "id": "nBUPzqwVUWTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the corpora"
      ],
      "metadata": {
        "id": "P4EkVZssWhcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deEmojify(text):\n",
        "    regex_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return regex_pattern.sub(r'',text)"
      ],
      "metadata": {
        "id": "t1ooidJ2Z3MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import sub, split, M\n",
        "\n",
        "with open(reqfile, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "cleancorp = \"\"\n",
        "linesdone = 0\n",
        "\n",
        "for line in lines:\n",
        "    line = sub(r'[\\.\\,\\(\\)\\-\\—\\”\\“\\:\\?\\n\\…\\।\\‘\\_\\–\\#\\¿\\?\\¡\\í\\!\\\\\\/\\á\\[\\]\\;\\&\\ó\\‘\\|\\%\\'a-zA-Z0-9\\—\\@\\(\\)\\-\\’\\=\\+\\\"\\>\\~\\»\\*]', '', line) \n",
        "    line = sub(r'^[\\s\\,\\.\\#\\/\\'\\\"\\’\\:\\\\\\/\\\"\\_]*$', '', line, flags=M)\n",
        "    line = sub(r'^[\\s]*$', '', line, flags=M)\n",
        "    line = deEmojify(line)\n",
        "    line = line.strip()\n",
        "    if len(line) == 0:\n",
        "      continue\n",
        "    print(line)\n",
        "    if len(cleancorp) > 0:\n",
        "      cleancorp = cleancorp +  \"\\n\" + line\n",
        "    else:\n",
        "      cleancorp = line\n",
        "    linesdone = linesdone + 1\n",
        "\n",
        "with open('/content/cleancorp.txt', 'w') as ff:\n",
        "  ff.write(cleancorp)\n",
        "\n",
        "print(\"Number of lines: \", linesdone)"
      ],
      "metadata": {
        "id": "eC5xSgXJY6Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting all the individual sentences into a list along with the cleaned corpora sentences "
      ],
      "metadata": {
        "id": "_ELJxhLNF_EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(reqfile, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "\n",
        "sent = []\n",
        "\n",
        "for line in lines:\n",
        "  sent.append(line)\n",
        "\n",
        "with open('/content/cleancorp.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "cleansent = []\n",
        "\n",
        "for line in lines:\n",
        "    cleansent.append(line)\n",
        "  \n",
        "print(len(sent))\n",
        "print(len(cleansent))"
      ],
      "metadata": {
        "id": "lgW8EDiDc1KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count the number of words per sentence"
      ],
      "metadata": {
        "id": "up_0flP5gCbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number = []\n",
        "\n",
        "for line in cleansent:\n",
        "  number.append(len(line.split()))\n",
        "\n",
        "print(number)"
      ],
      "metadata": {
        "id": "e4v_b65EgBb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plottting the distribution of words per sentence "
      ],
      "metadata": {
        "id": "nFHgCuyUh73N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "freq = Counter(number)\n",
        "print(freq)"
      ],
      "metadata": {
        "id": "DjPneF7ui36t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "data = np.array(number)\n",
        "plt.hist(data, bins=np.arange(data.min(), data.max()+1))\n",
        "plt.show"
      ],
      "metadata": {
        "id": "eQ2RR5NhiML0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {}\n",
        "\n",
        "for i in range(10, 501, 10):\n",
        "  dic[i] = 0\n",
        "\n",
        "for data in freq:\n",
        "  dic[((data // 10) * 10) + 10] += freq[data]\n",
        "\n",
        "print(dic)"
      ],
      "metadata": {
        "id": "4ZlFiXHJpA7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for thing in dic:\n",
        "  if dic[thing] != 0:\n",
        "    x.append(thing)\n",
        "    y.append(dic[thing])\n",
        "\n",
        "fig = plt.figure(figsize=(5,2))\n",
        "plt.xlabel(\"Values\")\n",
        "plt.ylabel(\"Frequencies\")\n",
        "plt.title(\"Plotting the word frequency\")\n",
        "plt.bar(x, y, width=0.5)\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zLPAurlVqecz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Tokenization\n"
      ],
      "metadata": {
        "id": "vzvgLKKTtWCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bn_nlp.tokenizer import wordTokenizer\n",
        "wordtoken=wordTokenizer()\n",
        "\n",
        "with open('/content/cleancorp.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "wordlistbeng = []\n",
        "linenum = 0\n",
        "wordcount = 0\n",
        "\n",
        "for line in lines:\n",
        "  linenum += 1\n",
        "  print(linenum)\n",
        "  tokens = wordtoken.basic_tokenizer(line)\n",
        "  wordcount = wordcount + len(tokens)\n",
        "  for word in tokens:\n",
        "    wordlistbeng.append(word)\n",
        "\n",
        "print(\"wordcount: \", wordcount)"
      ],
      "metadata": {
        "id": "LfVZxt9rtbCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding word frequency"
      ],
      "metadata": {
        "id": "fumxauPmuoe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "wordcountbeng = Counter(wordlistbeng)\n",
        "print(wordcountbeng.most_common(30))"
      ],
      "metadata": {
        "id": "Iq-1EiAruWJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the most frequent words "
      ],
      "metadata": {
        "id": "epcvGEbsusJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from bn_nlp.preprocessing import ban_processing\n",
        "bp=ban_processing()\n",
        "\n",
        "bengalicommonwords = []\n",
        "bengalicommonwordfreq = []\n",
        "\n",
        "for thing in wordcountbeng.most_common(30):\n",
        "  bengalicommonwords.append(bp.bn2enCon(thing[0]))\n",
        "  bengalicommonwordfreq.append(thing[1])\n",
        "\n",
        "fig = plt.figure(figsize=(35,6))\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequencies\")\n",
        "plt.title(\"Plotting the most commonly occuring words\")\n",
        "plt.bar(bengalicommonwords, bengalicommonwordfreq, width=0.5)\n",
        "plt.plot(bengalicommonwords,bengalicommonwordfreq)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qan9qlJhuup1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stop words "
      ],
      "metadata": {
        "id": "-czshxXPu0Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bn_nlp.preprocessing import ban_processing\n",
        "from bn_nlp.tokenizer import wordTokenizer\n",
        "\n",
        "bp=ban_processing()\n",
        "wordtoken=wordTokenizer()\n",
        "\n",
        "nostopbeng = []\n",
        "wordcountnostop = 0\n",
        "\n",
        "for line in cleansent:\n",
        "  wordy = bp.stop_word_remove(line)\n",
        "  tokens = wordtoken.basic_tokenizer(wordy)\n",
        "  wordcountnostop = wordcountnostop + len(tokens)\n",
        "  for word in tokens:\n",
        "    nostopbeng.append(word)\n",
        "\n",
        "print(\"wordcount: \", wordcountnostop)"
      ],
      "metadata": {
        "id": "BL7UWXlNu2rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing frequency of words without the stop words"
      ],
      "metadata": {
        "id": "HjaKwjVCu9Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "wordcountbeng = Counter(nostopbeng)\n",
        "print(wordcountbeng.most_common(30))"
      ],
      "metadata": {
        "id": "rOqLl5PmvFgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the most frequent words without the stop words"
      ],
      "metadata": {
        "id": "mH8RLDDWvKYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from bn_nlp.preprocessing import ban_processing\n",
        "bp=ban_processing()\n",
        "\n",
        "bengalicommonwords = []\n",
        "bengalicommonwordfreq = []\n",
        "\n",
        "for thing in wordcountbeng.most_common(30):\n",
        "  bengalicommonwords.append(bp.bn2enCon(thing[0]))\n",
        "  bengalicommonwordfreq.append(thing[1])\n",
        "\n",
        "fig = plt.figure(figsize=(35,5))\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequencies\")\n",
        "plt.title(\"Plotting the most commonly occuring words - without stop words\")\n",
        "plt.bar(bengalicommonwords, bengalicommonwordfreq, width=0.5)\n",
        "plt.plot(bengalicommonwords,bengalicommonwordfreq)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hObr4gV0vNx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging\n"
      ],
      "metadata": {
        "id": "3gWEDyH0v2tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data with Stop Words "
      ],
      "metadata": {
        "id": "_uXRrtnRxrYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import POS\n",
        "bn_pos = POS()\n",
        "\n",
        "model_path = \"/content/bn_pos.pkl\"\n",
        "\n",
        "bengalitagged = []\n",
        "\n",
        "for text in cleansent: #this one has stopwords \n",
        "  res = bn_pos.tag(model_path, text)\n",
        "\n",
        "  for thing in res:\n",
        "    print(thing)\n",
        "    bengalitagged.append(thing)"
      ],
      "metadata": {
        "id": "mwMV6QNev4h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the frequency of each POS tag"
      ],
      "metadata": {
        "id": "1o_yaZUdxids"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags = []\n",
        "\n",
        "for thing in bengalitagged:\n",
        "  tags.append(thing[1])\n",
        "\n",
        "tagcountbeng = Counter(tags)\n",
        "\n",
        "print(tagcountbeng)"
      ],
      "metadata": {
        "id": "Djf1wRssxb5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the 30 most common tags"
      ],
      "metadata": {
        "id": "i1kOfUgwxl9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tags = []\n",
        "freqs = []\n",
        "\n",
        "for thing in tagcountbeng.most_common(30):\n",
        "  tags.append(thing[0])\n",
        "  freqs.append(thing[1])\n",
        "\n",
        "fig = plt.figure(figsize=(25,5))\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequencies\")\n",
        "plt.title(\"Plotting the most commonly occuring POS tags\")\n",
        "plt.bar(tags, freqs, width=0.5)\n",
        "plt.plot(tags, freqs)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3w4lV74HxfsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data without stop words "
      ],
      "metadata": {
        "id": "iak4t6VVxuHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import POS\n",
        "bn_pos = POS()\n",
        "\n",
        "model_path = \"/content/bn_pos.pkl\"\n",
        "\n",
        "bengalitagged1 = []\n",
        "\n",
        "res = bn_pos.tag(model_path, nostopbeng)\n",
        "\n",
        "for thing in res:\n",
        "  print(thing)\n",
        "  bengalitagged1.append(thing)"
      ],
      "metadata": {
        "id": "UXqO5f1mxwaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the frequency of each POS tag"
      ],
      "metadata": {
        "id": "--99GYzVx0eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tags = []\n",
        "\n",
        "for thing in bengalitagged1:\n",
        "  tags.append(thing[1])\n",
        "\n",
        "tagcountbeng = Counter(tags)\n",
        "\n",
        "print(tagcountbeng)"
      ],
      "metadata": {
        "id": "8mwmfS5tx3W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the 30 most common tags "
      ],
      "metadata": {
        "id": "OVJBXFi2x44b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tags = []\n",
        "freqs = []\n",
        "\n",
        "for thing in tagcountbeng.most_common(30):\n",
        "  tags.append(thing[0])\n",
        "  freqs.append(thing[1])\n",
        "\n",
        "fig = plt.figure(figsize=(25,5))\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequencies\")\n",
        "plt.title(\"Plotting the most commonly occuring POS tags\")\n",
        "plt.bar(tags, freqs, width=0.5)\n",
        "plt.plot(tags, freqs)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jVZl_C_5x9ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming "
      ],
      "metadata": {
        "id": "YKO4unXOybCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bn_nlp.Stemmer import stemmerOP\n",
        "\n",
        "stemlist = []\n",
        "\n",
        "stemmer=stemmerOP()\n",
        "for word in nostopbeng:\n",
        "  print(word, \"->\", stemmer.stem(word))\n",
        "  stemlist.append(stemmer.stem(word))"
      ],
      "metadata": {
        "id": "q98zQQ_WygPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding the most common word stems "
      ],
      "metadata": {
        "id": "6g_emoxeym5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "stemcountbeng = Counter(stemlist)\n",
        "print(stemcountbeng.most_common(30))"
      ],
      "metadata": {
        "id": "Ms_L9Bq9yr_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the most common stems "
      ],
      "metadata": {
        "id": "7hi10n3jyv4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from bn_nlp.preprocessing import ban_processing\n",
        "bp=ban_processing()\n",
        "\n",
        "commonstems = []\n",
        "commonstemfreq = []\n",
        "\n",
        "for thing in stemcountbeng.most_common(30):\n",
        "  commonstems.append(bp.bn2enCon(thing[0]))\n",
        "  commonstemfreq.append(thing[1])\n",
        "\n",
        "fig = plt.figure(figsize=(35,5))\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequencies\")\n",
        "plt.title(\"Plotting the most commonly occuring stems\")\n",
        "plt.bar(commonstems, commonstemfreq, width=0.5)\n",
        "plt.plot(commonstems,commonstemfreq)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mg4MY7-tyyEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Cloud "
      ],
      "metadata": {
        "id": "DS-aYGamy3jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from bn_nlp.posTag import postag\n",
        "\n",
        "tagger=postag()\n",
        "wordcountbeng = Counter(nostopbeng)\n",
        "\n",
        "pos = []\n",
        "\n",
        "length = len(wordcountbeng)\n",
        "\n",
        "for thing in wordcountbeng.most_common(2000):\n",
        "  possy = tagger.tag(thing[0])\n",
        "  for possed in possy:\n",
        "    pos.append(possed[1])\n",
        "\n",
        "bengcommonwords = []\n",
        "bengcommonwordfreq = []\n",
        "\n",
        "index = 0\n",
        "i = 0\n",
        "\n",
        "for thing in wordcountbeng.most_common(2000):\n",
        "  if 'noun' == pos[index]:\n",
        "    bengcommonwords.append(thing[0])\n",
        "    bengcommonwordfreq.append(thing[1] * 5)\n",
        "    i += 1\n",
        "  elif 'adjective' in pos[index]:\n",
        "    bengcommonwords.append(thing[0])\n",
        "    bengcommonwordfreq.append(thing[1] * 3)\n",
        "    i += 1\n",
        "  elif 'adverb' in pos[index]:\n",
        "    bengcommonwords.append(thing[0])\n",
        "    bengcommonwordfreq.append(thing[1] * 2)\n",
        "    i += 1\n",
        "  if 'verb' == pos[index]:\n",
        "    bengcommonwords.append(thing[0])\n",
        "    bengcommonwordfreq.append(thing[1] * 2.5)\n",
        "    i += 1\n",
        "  \n",
        "  if i == 50:\n",
        "    break\n",
        "  index += 1"
      ],
      "metadata": {
        "id": "OnnkbNPXy5AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(bengcommonwords)\n",
        "\n",
        "for i in range(n-1):\n",
        "  for j in range(0, n-i-1):\n",
        "    if bengcommonwordfreq[j] < bengcommonwordfreq[j + 1] :\n",
        "            bengcommonwordfreq[j], bengcommonwordfreq[j + 1] = bengcommonwordfreq[j + 1], bengcommonwordfreq[j]\n",
        "            bengcommonwords[j], bengcommonwords[j + 1] = bengcommonwords[j + 1], bengcommonwords[j]"
      ],
      "metadata": {
        "id": "Jh4dvkaGzAPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from bn_nlp.preprocessing import ban_processing\n",
        "bp=ban_processing()\n",
        "\n",
        "bengplot = []\n",
        "\n",
        "for word in bengcommonwords:\n",
        "  bengplot.append(bp.bn2enCon(word))\n",
        "\n",
        "fig = plt.figure(figsize=(40,5))\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequencies\")\n",
        "plt.title(\"Plotting the word cloud words\")\n",
        "plt.bar(bengplot, bengcommonwordfreq, width=0.5)\n",
        "plt.plot(bengplot, bengcommonwordfreq)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ng5lH2eEzDLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(bengcommonwords)):\n",
        "  print(bengcommonwords[i])"
      ],
      "metadata": {
        "id": "7th39XM0zFJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Cloud Visualization"
      ],
      "metadata": {
        "id": "fCxxm6YPzHbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bn_nlp.preprocessing import ban_processing\n",
        "bp=ban_processing()\n",
        "\n",
        "wcdict = {}\n",
        "\n",
        "for i in range(len(bengcommonwords)):\n",
        "  wcdict[bp.bn2enCon(bengcommonwords[i])] = bengcommonwordfreq[i]\n",
        "\n",
        "print(wcdict)"
      ],
      "metadata": {
        "id": "FOyLdKOdzewl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(background_color='black', width = 500, height=500, margin=2)"
      ],
      "metadata": {
        "id": "_mm9RxAhziVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc.fit_words(wcdict)\n",
        "wc.to_file('wc.png')"
      ],
      "metadata": {
        "id": "2aq_4GL0zk00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wcdict = {}\n",
        "\n",
        "for i in range(len(bengcommonwords)):\n",
        "  wordy = bengcommonwords[i]\n",
        "  wcdict[wordy] = bengcommonwordfreq[i]\n",
        "\n",
        "print(wcdict)"
      ],
      "metadata": {
        "id": "l_yxgvkozxEM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}